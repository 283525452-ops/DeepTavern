# core/harvester/scheduler.py
import threading
import queue
import time
from core.harvester.crawler import WebCrawler
from core.harvester.cleaner import LocalCleaner
from core.database.vector_store import VectorStore
from core.utils.logger import logger

class KnowledgeHarvester(threading.Thread):
    def __init__(self):
        super().__init__()
        self.name = "HarvesterThread"
        self.daemon = True
        self.queue = queue.PriorityQueue()
        self.running = True
        
        self.crawler = WebCrawler()
        self.cleaner = LocalCleaner()
        self.vec = VectorStore(collection_name="long_term_memory")

        # ç™½åå•/é»‘åå•ä¿æŒä¸å˜...
        self.whitelist = ["wikipedia.org", "baike.baidu.com", "zhihu.com", "gamersky.com", "ali213.net"]
        self.blacklist = ["csdn.net", "baidu.com/link", "weibo.com", "bilibili.com"]

    def add_task(self, keyword, priority=10):
        if keyword:
            logger.info(f"[Harvester] ğŸ“¥ Added task: {keyword}")
            self.queue.put((priority, time.time(), keyword))

    def run(self):
        logger.info("[Harvester] Service Started (Batch Aggregation Mode).")
        while self.running:
            try:
                priority, _, keyword = self.queue.get(timeout=5)
                self._process_task_batch(keyword) # æ”¹ç”¨ Batch æ–¹æ³•
                self.queue.task_done()
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"[Harvester] Loop Error: {e}")
                time.sleep(5)

    def _process_task_batch(self, keyword):
        # 1. çˆ¬å–å¤šæ¡ (æ¯”å¦‚ä¸€æ¬¡æŠ“ 4 ä¸ªç½‘é¡µ)
        raw_results = self.crawler.search_and_fetch(
            keyword, 
            whitelist=self.whitelist, 
            blacklist=self.blacklist,
            max_results=6  # å¢åŠ æ•°é‡ï¼Œå–‚é¥± LLM
        )
        
        if not raw_results:
            return

        # 2. å‡†å¤‡æ•°æ®
        contents_to_merge = []
        for res in raw_results:
            # ç®€å•è¿‡æ»¤å¤ªçŸ­çš„åƒåœ¾
            if len(res['content']) > 200:
                contents_to_merge.append({
                    'source': res['domain'],
                    'text': res['content']
                })

        if not contents_to_merge:
            logger.warning("[Harvester] No valid content to merge.")
            return

        # 3. èšåˆæ¸…æ´— (One Pass)
        logger.info(f"[Harvester] ğŸ§  Synthesizing {len(contents_to_merge)} pages for '{keyword}'...")
        final_summary = self.cleaner.clean_batch(contents_to_merge, keyword)

        if final_summary:
            # 4. å­˜å…¥å‘é‡åº“ (åªå­˜è¿™ä¸€æ¡é«˜è´¨é‡çš„)
            mem_id = f"lore_{int(time.time())}_{hash(keyword) % 10000}"
            
            # æ„é€ å…ƒæ•°æ®ï¼Œè®°å½•æ‰€æœ‰æ¥æº
            sources_str = ", ".join([c['source'] for c in contents_to_merge])
            
            self.vec.add_memory(
                text=final_summary, 
                metadata={
                    "type": "INTERNET_LORE", 
                    "keyword": keyword,
                    "sources": sources_str,
                    "timestamp": str(int(time.time())),
                    "quality": "high_batch" # æ ‡è®°ä¸ºé«˜è´¨é‡èšåˆ
                }, 
                memory_id=mem_id
            )
            logger.info(f"[Harvester] âœ… Saved Deep Lore for '{keyword}' (Length: {len(final_summary)})")
        else:
            logger.warning("[Harvester] Batch summary failed.")
