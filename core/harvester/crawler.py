# core/harvester/crawler.py
import requests
import time
import random
import trafilatura
from duckduckgo_search import DDGS
from bs4 import BeautifulSoup # æˆ‘ä»¬éœ€è¦æŠŠ BS4 è¯·å›æ¥ä¸“é—¨è§£æ Bing çš„æœç´¢ç»“æœé¡µ
from core.utils.logger import logger
import urllib3

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class WebCrawler:
    def __init__(self):
        # =====================================================
        # [å¯é€‰] å¦‚æœä½ æœ‰ä»£ç† (å¦‚ v2ray/clash)ï¼Œè¯·åœ¨è¿™é‡Œå¡«å…¥
        # ä¾‹å¦‚: proxies = {"http": "http://127.0.0.1:7890", "https": "http://127.0.0.1:7890"}
        # å¦‚æœæ²¡æœ‰ä»£ç†ï¼Œä¿æŒä¸º None å³å¯ï¼Œä»£ç ä¼šè‡ªåŠ¨é™çº§åˆ° Bing
        # =====================================================
        self.proxies = None 
        
        try:
            self.ddgs = DDGS(proxy=self.proxies['http'] if self.proxies else None, timeout=10)
        except:
            self.ddgs = None

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
        }

    def _fetch_via_jina(self, url):
        """ç­–ç•¥A: ä½¿ç”¨ Jina Reader (æŠ—åçˆ¬ + è½¬Markdown)"""
        jina_url = f"https://r.jina.ai/{url}"
        try:
            # Jina éœ€è¦è®¿é—®å¤–ç½‘ï¼Œå¦‚æœæœ¬åœ°æœ‰ä»£ç†æœ€å¥½åŠ ä¸Šï¼Œæ²¡æœ‰ä¹Ÿèƒ½è·‘(JinaæœåŠ¡å™¨åœ¨æµ·å¤–)
            resp = requests.get(jina_url, headers=self.headers, timeout=30)
            if resp.status_code == 200:
                text = resp.text
                if len(text) > 200 and "Cloudflare" not in text:
                    return text
        except Exception as e:
            logger.debug(f"[Crawler] Jina fetch failed: {e}")
        return None

    def _fetch_via_local(self, url):
        """ç­–ç•¥B: æœ¬åœ° Requests + Trafilatura (æœ¬åœ°ç›´è¿)"""
        try:
            resp = requests.get(url, headers=self.headers, timeout=15, verify=False)
            if resp.status_code == 200:
                # è‡ªåŠ¨ä¿®æ­£ç¼–ç 
                if resp.encoding == 'ISO-8859-1':
                    resp.encoding = resp.apparent_encoding
                
                # æå–æ­£æ–‡
                text = trafilatura.extract(
                    resp.text, 
                    include_comments=False, 
                    include_tables=True, 
                    include_formatting=True, # ä¿ç•™ Markdown æ ¼å¼
                    no_fallback=True
                )
                return text
        except Exception as e:
            logger.debug(f"[Crawler] Local fetch failed: {e}")
        return None

    def _search_ddg(self, keyword, max_results):
        """å¼•æ“ 1: DuckDuckGo"""
        links = []
        if not self.ddgs: return []
        try:
            logger.info(f"[Crawler] ğŸ” Searching via DuckDuckGo...")
            results = self.ddgs.text(keyword, region='cn-zh', max_results=max_results+2)
            for r in results:
                links.append({'href': r['href'], 'title': r['title']})
        except Exception as e:
            logger.warning(f"[Crawler] DDG failed (Network Issue?): {e}")
        return links

    def _search_bing(self, keyword, max_results):
        """å¼•æ“ 2: Bing CN (å›½å†…ç›´è¿)"""
        links = []
        try:
            logger.info(f"[Crawler] ğŸ” Fallback to Bing CN...")
            url = f"https://cn.bing.com/search?q={keyword}"
            resp = requests.get(url, headers=self.headers, timeout=10, verify=False)
            
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                # è§£æ Bing çš„åˆ—è¡¨ç»“æ„
                items = soup.find_all('li', class_='b_algo')
                for item in items:
                    h2 = item.find('h2')
                    if h2:
                        a_tag = h2.find('a')
                        if a_tag and a_tag.get('href'):
                            links.append({
                                'href': a_tag['href'],
                                'title': a_tag.get_text()
                            })
                            if len(links) >= max_results + 2: break
        except Exception as e:
            logger.error(f"[Crawler] Bing search failed: {e}")
        return links

    def search_and_fetch(self, keyword, whitelist=[], blacklist=[], max_results=3):
        # 1. æœç´¢é˜¶æ®µ (åŒå¼•æ“)
        search_links = self._search_ddg(keyword, max_results)
        
        # å¦‚æœ DDG æŒ‚äº†ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° Bing
        if not search_links:
            search_links = self._search_bing(keyword, max_results)

        if not search_links:
            logger.warning("[Crawler] All search engines failed.")
            return []

        # 2. ç­›é€‰é˜¶æ®µ
        candidates = []
        for item in search_links:
            url = item['href']
            # ç®€å•çš„åŸŸåæå–
            try:
                domain = url.split('/')[2]
            except:
                domain = ""
            
            if any(black in domain for black in blacklist): continue
            
            score = 50
            if any(white in domain for white in whitelist): score = 100
            
            candidates.append((score, item))
        
        candidates.sort(key=lambda x: x[0], reverse=True)
        targets = candidates[:max_results]
        
        logger.info(f"[Crawler] ğŸ¯ Targets: {[t[1]['title'][:10] for t in targets]}")
        
        results = []

        # 3. æŠ“å–é˜¶æ®µ (æ··åˆç­–ç•¥)
        for _, item in targets:
            url = item['href']
            title = item['title']
            domain = url.split('/')[2] if '//' in url else url
            
            time.sleep(random.uniform(1, 3))
            
            # ä¼˜å…ˆ Jina (äº‘ç«¯)
            content = self._fetch_via_jina(url)
            source_type = "Jina-Reader"
            
            # å¤±è´¥åˆ™æœ¬åœ° Trafilatura
            if not content:
                content = self._fetch_via_local(url)
                source_type = "Local-Trafilatura"

            if content and len(content) > 50: # æ”¾å®½é™åˆ¶ï¼Œæœ‰äº›çŸ­è®¾å®šä¹Ÿå¾ˆæœ‰ç”¨
                logger.info(f"[Crawler] âœ… Fetched [{source_type}]: {title[:15]}... ({len(content)} chars)")
                results.append({
                    "title": title,
                    "url": url,
                    "content": content,
                    "domain": domain
                })
            else:
                logger.warning(f"[Crawler] âŒ Content empty: {url}")

        return results
